# 分布式共识协议

CAP原理: 一致性(Consistency), 可用性(Availability), 分区容错性(Partition Tolerance)



## 一致性模型

弱一致性
    + 最终一致性
        * DNS
        * Gossip(Cassandra通讯协议)
- 强一致性
    + 同步
    + Paxos
    + [Etcd](#Raft算法)

> 分布式系统对fault tolerance的一般解决方案是state machine replication



## 2PC(2PC Phase Commit)

引入一个“协调者”的组件来统一调度所有分布式节点的执行。

**阶段一（投票阶段）**：提交事务请求
**阶段二（执行阶段）**：执行事务提交

优点：原理简单、实现方便
缺点：同步阻塞、单点问题、数据不一致



## 3PC（3 Phase Commit）

省略



# Raft算法

主要分为三个部分：[选主](#Leader选举)，日志复制，安全性。

[完整动画演示路径](http://thesecretlivesofdata.com/raft/)

## 角色定义

 Leader, Follower, Candidate

![](https://d.pr/free/i/wtgP4Z+)



## Leader选举

Leader向所有`Followers`周期性发送`heartbeat`。如果Follower在选举**超时时间(timeout)**内没有收到Leader的heartbeat，就会**等待随机时间**后发起一次Leader选举。

> Hearbeat携带`选举信息`和`写复制信息`。



### 1）Follower如何发起选举操作？

`Follower`在选举**超时时间**内未收到`Leader`的**heartbeat**，将其当前`term + 1`然后转换成`Candidate`.

结果又三种情况发生：  
1. 赢得多数选票， 成功选举为Leader；发送心跳给Follwer.
2. 选举期间，收到Leader消息，表示有其他服务器抢先当选Leader. 
3. 没有服务器赢得多数的选票，Leader选举失败; 等待**选举超时**后发起下一次选举。



![Alt Text](https://d.pr/free/i/j6GK27+)

`Leader`通过定期向所有`Followers`发送心跳信息维持其统治。

 ![](https://d.pr/free/i/ozizIi+)

### 2）Follower如何投票？

`Follower`在收到`Candidate`选举消息, 进行合法性检查(**term值大于当前自身term值**),在同一term中只会按照先到先得投票给**至多一个**`candidate`.

> candicate 永远都投票给自己。

### 3）什么资格才能成为Candidate？
拥有最新已提交`Log entry` 的Follower才有资格成为Leader。`Candidate`在 发起选举（RequestVote RPC）时，要带上自己的最后一条日志的`term`和`log index`. 其他节点收到消息时，如果发现自己的日志比请求中携带的更新，则拒绝投票。

### 4) 发生分区错误，如何保证在最终一致性？
网络问题提集群分成多个区，这里以2区为例。`R(5)` -> `R(2)`和`R(3)`.

### 情况1：两个`Leader`
- R(2)存在`Leader`和Follower，Follower收到Leader周期性心跳，正常读
- R（3）重新发起选举，最终Term+1，多数派赞成，Leader选举成功，正常读写

**Case1** : `Client`发送write请求给`R(2)`，R(2)的LeaderWAL后发送给`Follower`，只收到一个Commit未超过半数，最终`Commit Failed`写失败。

**Case2** :  `Client`发送write请求给`R(3)`，R(3)的LeaderWAL后发送给`Follower`，Commit超过半数，最终`Commit Succ`写成功。

**Case3** ：R(2)和R(3)连接成功，R(2)Leader收到R(3)的合法性心跳(term)，退换成`Follower`, Log replication最终一致性



### 5）如何保证Term全局有序递增切不重复？

Term初始化从`0`开始, 64位长整型，`timestamp_hostname_nonce`. 高位使用时间戳保证全局递增，`hostname`保证不重复, `once`用避免并发冲突.

> nonce 可以采用60s内不重复的随机数.
> 



## 日志复制 or 数据同步是如何做到的？

WAL机制可以保证数据不丢. 数据同步通过Log来进行，log通过周期性**心跳携带传输**.
- commitIndex： 已知的被提交的最大日志条目的索引值（从 0 开始递增）
- lastApplied： 被状态机执行的最大日志条目的索引值（从 0 开始递增）
- log[]： 日志条目；每个条目包含状态机的要`执行命令`和从领导人处收到时任期号`term`

![](https://d.pr/free/i/kdE1n5+)



### WAL 与 snapshot 在 etcd 中的命名规则

在 etcd 的数据目录中，WAL 文件以`$seq-$index.wal`的格式存储。最初始的 WAL 文件是``0000000000000000-0000000000000000.wal`，表示是所有 WAL 文件中的第 0 个，初始的 Raft 状态编号为 0。运行一段时间后可能需要进行日志切分，把新的条目放到一个新的 WAL 文件中。

snapshot 的存储命名则比较容易理解，以`$term-$index.wal`格式进行命名存储。term 和 index 就表示存储 snapshot 时数据所在的 raft 节点状态，当前的任期编号以及数据项位置信息。

etcd 默认每 10000 条记录做一次 snapshot.



## 安全性

### 如何保证最短时间内竞选出Leader，防止竞选冲突？

随机**timeout**，最小概率出现竞选冲突。



### 如何防止别的Candidate在遗漏部分数据的情况下发起投票成为Leader？

Raft竞选的机制中，使用随机值决定超时时间，第一个超时的节点就会提升term编号发起新一轮投票，一般情况下别的节点收到竞选通知就会投票。但是， 如果发起竞选的节点在上一个term中保存的**已提交数据不完整，节点就会拒绝投票给它**。通过这种机制就可以防止遗漏数据的节点成为Leader。



### Raft某个节点宕机后会如何？

- 如果是Follower节点宕机，如果剩余可用节点数量超过半数，集群可以几乎没有影响的正常工作。
- 如果是Leader节点宕机，那么Follower就收不到心跳而超时，发起竞选获得投票，成为新一轮term的Leader，继续为集群提供服务

> 需要注意的是；etcd目前没有任何机制会`自动`去变化整个集群总共的节点数量，即如果没有人为的调用API，etcd宕机后的节点仍然被计算为总节点数中，任何请求被确认需要获得的投票数都是这个总数的半数以上。

![](https://d.pr/free/i/kyxPJo+)



### 为什么Raft算法在确定可用节点数量时不需要考虑拜占庭将军问题？

**etcd中假设所有的节点都是`诚实`的**。etcd在竞选前需要告诉别的节点自身的term编号以及前一轮term最终结束时的index值，这些数据都是准确的，其他节点可以根据这些值决定是否投票。另外，etcd严格限制Leader到Follower这样的数据流向保证数据一致不会出错。



## Etcd实现的Raft算法性能如何？

单实例节点支持每秒1000次数据写入。节点越多，由于数据同步涉及到网络延迟，会根据实际情况越来越慢，而读性能会随之变强，因为每个节点都能处理用户请求。



## 相关阅读

[etcd：从应用场景到实现原理的全方位解读](https://www.infoq.cn/article/etcd-interpretation-application-scenario-implement-principle)

[Raft算法详解](https://zhuanlan.zhihu.com/p/32052223)

[Raft 一致性算法论文译文](https://www.infoq.cn/article/raft-paper)






















